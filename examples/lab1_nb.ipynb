{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c019fb87-41a0-4e0c-aba5-9d7be818538f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Module 1 Labs Jupyter Notebook\n",
    "\n",
    "This notebook explores models based on an image classification model for the Fashion-MNIST dataset using tf.keras. Extra code has been added for the purposes of this lab.\n",
    "\n",
    "Based on: https://www.tensorflow.org/tutorials/keras/classification.\n",
    "\n",
    "\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2017 Fran√ßois Chollet\n",
    "\n",
    " Permission is hereby granted, free of charge, to any person obtaining a\n",
    " copy of this software and associated documentation files (the \"Software\"),\n",
    " to deal in the Software without restriction, including without limitation\n",
    " the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    " and/or sell copies of the Software, and to permit persons to whom the\n",
    " Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    " The above copyright notice and this permission notice shall be included in\n",
    " all copies or substantial portions of the Software.\n",
    "\n",
    " THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    " IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    " FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    " THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    " LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    " FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    " DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "## Module 1-Lab 1\n",
    "\n",
    "### Import modules and define functions\n",
    "Run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274dc38f-a4da-4be1-bf55-8caedea3d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import gzip\n",
    "import tempfile\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "print(\"TensorFlow version is \"+tf.__version__+\".\")\n",
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  true_label, img = true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  true_label = true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def ers_predict(model, input):\n",
    "    x = model(input)\n",
    "    return x\n",
    "set = 0 \n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "start_image = (set*num_images)\n",
    "end_image = (set*num_images)+num_images\n",
    "print(\"Display functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefe1b7-6d13-4a9e-abec-281982bcd056",
   "metadata": {},
   "source": [
    "### Load fashion images for the model to classify\n",
    "\n",
    "The next cell sets the names for the classified images. It then defines a function for loading validation data, which are images that will rull through the model. Finally it loads those images.\n",
    "Run the cell now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c2380-5f65-45f6-825c-afcb05cf3737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "def load_validation_data():\n",
    "    \"\"\"Loads the Fashion-MNIST dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of Numpy arrays: `(x_test, y_test)`.\n",
    "\n",
    "    License:\n",
    "        The copyright for Fashion-MNIST is held by Zalando SE.\n",
    "        Fashion-MNIST is licensed under the [MIT license](\n",
    "        https://github.com/zalandoresearch/fashion-mnist/blob/master/LICENSE).\n",
    "\n",
    "    \"\"\"\n",
    "    base = \"/tmp/data/\"\n",
    "    files = [\n",
    "        \"t10k-labels-idx1-ubyte.gz\",\n",
    "        \"t10k-images-idx3-ubyte.gz\",\n",
    "    ]\n",
    "\n",
    "    paths = []\n",
    "    for fname in files:\n",
    "        paths.append(base + fname)\n",
    "\n",
    "    with gzip.open(paths[0], \"rb\") as lbpath:\n",
    "        y_test = np.frombuffer(lbpath.read(), np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(paths[1], \"rb\") as imgpath:\n",
    "        x_test = np.frombuffer(imgpath.read(), np.uint8, offset=16).reshape(len(y_test), 28, 28)\n",
    "        \n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "test_images, test_labels = load_validation_data()\n",
    "test_images = test_images / 255.0\n",
    "print(\"Images loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f189af-16a3-4afe-96b7-7a6682659ef2",
   "metadata": {},
   "source": [
    "### Define the untrained model\n",
    "Note that model has: \n",
    "- 1 Flatten layer, which specifies an implicit input layer and then \"flattens\" the array of pixels into a 1-dimensional array\n",
    "- 1 hidden (Dense) layer \n",
    "- 1 output (Dense) layer, which has the same number of nodes as there are classes for images\n",
    "\n",
    "Run the cell below. (You can ignore the error, which indicates that this environment does not have any GPUs. That is okay for \"inferencing\" or showing the model in action. The actual training environment does have GPUs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1098d-dca4-4c83-a537-ecadcc8d84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(10),\n",
    "            ]\n",
    ")\n",
    "print(\"Untrained model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1c0854-2867-4353-8e79-5e29d1cb2de9",
   "metadata": {},
   "source": [
    "### Run images through the untrained model \n",
    "Next you will run images through the untrained model. \n",
    "You will see the results below the cell. In this exercise, when the model processes an image, it determines a probability for the image belonging to each of the 10 classes. It then assigns the image to the class with the highest probability. You will see a  bar chart to the right of each image, which shows these probabiblities. Under each image, you will see the class to which the model assigns the image and the associated probability. The text is red if the class is incorrect and blue if it is correct. In parentheses, the results show the correct class. \n",
    "\n",
    "Run the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cca3fa-fd37-49b8-bc19-04c0ad6c01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([ann_model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(test_images)\n",
    "num_rows = 8\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "print(\"\\033[1m\"+\"Untrained model classifications\"+\"\\033[0m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d0f49-29f1-445a-8791-e01564d42d37",
   "metadata": {},
   "source": [
    "### Stop\n",
    "As you see, the untrained model is not very successful at classifying images.\n",
    "\n",
    "You will now examine the Lab1_Constant experiment, which has already trained this same model. \n",
    "Return to the Module 1-Lab 1, Task 4 instructions. But leave this notebook open.\n",
    "\n",
    "### Start here at Module 1-Lab 1 Task 5\n",
    "\n",
    "### Load and use the trained model from \"Lab1_Constant\"\n",
    "You explored the \"Lab1_Constant\" experiment in the Web UI. As you saw, the model accuracy improved over the course of the training until it achieved about 88% accuracy. You will now load the trained model and see it in action. \n",
    "\n",
    "Run the cell below. \n",
    "As you will see, the model is now successfully classifying most images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aabd6b-ff59-4623-8d9c-3123da32c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model.load_weights(\"/tmp/determined-checkpoint/adaabc99-d2ae-4fbc-9003-db1444e364a9/determined-keras-model-weights\").expect_partial()\n",
    "print(\"\\033[1m\"+\"Trained model loaded.\"+\"\\033[0m\")\n",
    "probability_model = tf.keras.Sequential([ann_model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(test_images)\n",
    "num_rows = 8\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "print(\"\\033[1m\"+\"Trained model classifications\"+\"\\033[0m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146e929-f024-4e2f-b4a8-326640c48aab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Examine a Convolutional Neural Network (CNN)\n",
    "In the code below, examine a CNN model, defined for the same image classification purposes as the previous. This CNN model has: \n",
    "- 2 hidden Conv2D layers: \n",
    "  - The first layer both defines the input shape (creating an implicit input layer) and defines a convolutional layer with 64 3x3 filters\n",
    "  - The second defines a convolutional layer with 128 3x3 filters \n",
    "- Two layers that reformat the outputs for the next layer (MaxPool2D and Flatten) \n",
    "- 1 hidden Dense layer, which has 128 nodes \n",
    "- 1 output (Dense) layer, which has the same number of nodes as there are classes for images\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49568316-8f05-44ce-8d8f-a20770ae5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Conv2D(64, (3,3), input_shape=(28,28,1), padding=\"same\", activation=\"relu\"),\n",
    "                tf.keras.layers.Conv2D(128, (3,3), padding=\"same\", activation=\"relu\"),\n",
    "                tf.keras.layers.MaxPool2D(2,2),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(10),\n",
    "            ]\n",
    ")\n",
    "print(\"Untrained model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f621b-93e5-4e89-8e77-98942bbc4ef9",
   "metadata": {},
   "source": [
    "### Stop \n",
    "\n",
    "Return to the Module 1-Lab 1 instructions (Task 6, step 3).\n",
    "\n",
    "### Start here after examining the Lab1_CNN_Constant experiment\n",
    "\n",
    "### Load and use the trained model from \"Lab1_CNN_Constant\"\n",
    "You explored the \"Lab1_CNN_Constant\" experiment in the Web UI. This training process used the same model defined above. By the end of the process, it had created a model with about 92% accuracy as opposed to about 88% with the non-CNN ANN. You will now load the model trained in this way. \n",
    "\n",
    "Run the cell below. As you see, the model is now successfully classifying more of the images than before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100875b-f70e-4fef-8da1-ef1dc8e9686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.load_weights(\"/tmp/determined-checkpoint/de0bdc59-a8a9-4ce6-bfe0-a833a919546a/determined-keras-model-weights\").expect_partial()\n",
    "print(\"\\033[1m\"+\"Trained CNN model loaded.\"+\"\\033[0m\")\n",
    "ch_test_images = np.expand_dims(test_images, axis=-1)\n",
    "print(\"Channel dimension added to grayscale images to ensure proper input in the CNN.\")\n",
    "probability_model = tf.keras.Sequential([cnn_model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(np.expand_dims(test_images, -1))\n",
    "num_rows = 8\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, ch_test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "print(\"\\033[1m\"+\"Trained CNN model classifications\"+\"\\033[0m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f0794-a204-4d2c-848e-7acce786b06a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Explore the CNN layers and filters\n",
    "You will now explore the convolutional layers and their filters in a bit more detail.\n",
    "\n",
    "#### Explore a filter\n",
    "Run the cell below to:\n",
    "- See one of the Conv2D layer 1 filters, which was created during the training process\n",
    "- See what the image output by this \"looks like\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abf250-b1ce-48d6-a2aa-12bd1bf9e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+\"Filter 1 of Conv2D layer 1\"+\"\\033[0m\")\n",
    "filters, biases = cnn_model.layers[0].get_weights()\n",
    "f = filters[:, :, :, 0]\n",
    "plt.figure(figsize=(1.5,1.5))\n",
    "plt.imshow(f[:, :, :], cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\"+\"Example Test Image\"+\"\\033[0m\")\n",
    "img = ch_test_images[10]\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(img, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "batch_img = tf.expand_dims(img, axis=0)\n",
    "\n",
    "print(\"\\033[1m\"+\"The filter slides across the complete image and outputs a 'feature map' with its results.\"+\"\\033[0m\")\n",
    "\n",
    "print(\"\\033[1m\"+\"Here's what the output from this filter looks like.\"+\"\\033[0m\")\n",
    "\n",
    "conv_model = tf.keras.Sequential([cnn_model.layers[0]]) \n",
    "feature_map = conv_model.predict(batch_img)\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.imshow(feature_map[0, :, :, 0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953d3b8-be99-427b-93e7-974b18989919",
   "metadata": {},
   "source": [
    "#### More exploring filters\n",
    "The auto-trained filters help the CNN pick out various relevant features. The processing can continue over multiple layers.\n",
    "\n",
    "Run the cell below to see:\n",
    "- All 64 filters in the Conv2D layer 1\n",
    "- All the images output by these filters\n",
    "- All the images output after passing through the Conv2D layer 2 filters\n",
    "- All the images output after passing through the MaxPool2D layer, which reduces the size of the images and helps generalize key features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19d18c-4436-4fc3-9887-53d792c44e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+\"All filters of Conv2D layer 1\"+\"\\033[0m\")\n",
    "filters1, biases = cnn_model.layers[0].get_weights()\n",
    "num_rows = 8\n",
    "num_cols = 8\n",
    "num_images = num_rows * num_cols\n",
    "plt.figure(figsize=(24,24))\n",
    "for i in range(num_images):\n",
    "    f = filters1[:, :, :, i]\n",
    "    ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(f[:, :, :], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\"+\"Images after passing through Conv2D layer 1\"+\"\\033[0m\")\n",
    "print(\"\\033[1m\"+\"As you see, different filters highlight different features such as edges.\"+\"\\033[0m\")\n",
    "plt.figure(figsize=(24,24))\n",
    "for i in range(num_images):\n",
    "    ax = plt.subplot(num_rows, num_cols, i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(feature_map[0, :, :, i], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\"+\"Images after passing through Conv2D layer 2\"+\"\\033[0m\")\n",
    "conv2_model = tf.keras.Sequential([cnn_model.layers[0], cnn_model.layers[1]]) \n",
    "feature_map2 = ers_predict(conv2_model, batch_img)\n",
    "plt.figure(figsize=(24,12))\n",
    "for i in range(num_images*2):\n",
    "    ax = plt.subplot(num_rows, num_cols*2, i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(feature_map2[0, :, :, i], cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\"+\"Images after passing through MaxPool2D layer\"+\"\\033[0m\")\n",
    "print(\"\\033[1m\"+\"The images might look less clear to you. But the CNN is learning general features that indicate 'coat.'\"+\"\\033[0m\")\n",
    "pooling_model = tf.keras.Sequential([cnn_model.layers[0], cnn_model.layers[1], cnn_model.layers[2]]) \n",
    "feature_map3 = ers_predict(pooling_model, batch_img)\n",
    "plt.show()\n",
    "plt.figure(figsize=(24,12))\n",
    "for i in range(num_images*2):\n",
    "    ax = plt.subplot(num_rows, num_cols*2, i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(feature_map3[0, :, :, i], cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518ba022-e0f0-4337-9b0f-646427295da5",
   "metadata": {},
   "source": [
    "#### Continue to explore CNNs\n",
    "Run the cell below to see how the output of the MaxPool2D layer for two different images of coats. \n",
    "Notice various filtered images resemble each other. \n",
    "From here, the example CNN is ready to flatten the images and send them through a hidden and dense layer, as with the earlier ANN you examined.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2acb27-5b18-477e-bbc7-45ba0710d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[1m\"+\"Example Coat 1 and Example Coat 2\"+\"\\033[0m\")\n",
    "img2 = ch_test_images[6]\n",
    "plt.figure(figsize=(14,28))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.imshow(img, cmap=plt.cm.binary)\n",
    "az = plt.subplot(1, 2, 2)\n",
    "az.set_xticks([])\n",
    "az.set_yticks([])\n",
    "plt.imshow(img2, cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\"+\"Example Coat 1 after passing through both Conv2 layers and the MaxPool2D layer\"+\"\\033[0m\")\n",
    "plt.figure(figsize=(24,12))\n",
    "for i in range(num_images*2):\n",
    "    ax = plt.subplot(num_rows, num_cols*2, i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(feature_map3[0, :, :, i], cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\"+\"Example Coat 2 after passing through both Conv2 layers and the MaxPool2D layer\"+\"\\033[0m\")\n",
    "print(\"\\033[1m\"+\"Note the commonalities\"+\"\\033[0m\")\n",
    "batch_img2 = tf.expand_dims(img2, axis=0)\n",
    "feature_map4 = ers_predict(pooling_model, batch_img2)\n",
    "plt.figure(figsize=(24,12))\n",
    "for i in range(num_images*2):\n",
    "    ax = plt.subplot(num_rows, num_cols*2, i+1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.imshow(feature_map4[0, :, :, i], cmap='gray')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\033[1m\"+\"The CNN model can correctly classify both images.\"+\"\\033[0m\")\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.subplot(1, 4, 1)\n",
    "plot_image(10, predictions[10], test_labels, ch_test_images)\n",
    "plt.subplot(1, 4, 2)\n",
    "plot_value_array(10, predictions[10], test_labels)\n",
    "plt.subplot(1, 4, 3)\n",
    "plot_image(6, predictions[6], test_labels, ch_test_images)\n",
    "plt.subplot(1, 4, 4)\n",
    "plot_value_array(6, predictions[6], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99454854-bbf2-45f0-859f-a63ef4f4e6aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stop \n",
    "\n",
    "You have completed Module 1-Lab 1.\n",
    "\n",
    "## Module 1-Lab 2\n",
    "\n",
    "The code below redefines the model with the architecture determined by the HPO process. It then loads the trained model from the best checkpointed model created during the \"Lab1_Adaptive\" experiment. It then runs the images through the trained model and displays the results. Again you should see good performance.\n",
    "\n",
    "Run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8125060-1665-401b-9197-43df992d12e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cnn_adaptive_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(39, (3,3), input_shape=(28,28,1), padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv2D(94, (3,3), padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.336),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(497, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.287),\n",
    "    tf.keras.layers.Dense(10),])     \n",
    "cnn_adaptive_model.load_weights(\"/tmp/determined-checkpoint/0e250b7f-9a20-47a6-9162-08e62e3f2a2f/determined-keras-model-weights\").expect_partial()\n",
    "print(\"\\033[1m\"+\"Best model discovered in Adaptive ASHA experiment loaded\"+\"\\033[0m\")\n",
    "probability_model = tf.keras.Sequential([cnn_adaptive_model, \n",
    "                                         tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(ch_test_images)\n",
    "num_rows = 8\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "print(\"\\033[1m\"+\"Classifications by CNN model trained with Adaptive ASHA\"+\"\\033[0m\")\n",
    "print(\"\\033[1m\"+\"This model has an accuracy over 93% and is now classifying even more images correctly.\"+\"\\033[0m\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721fa077-6174-43e6-9d3d-e49aa2c5931e",
   "metadata": {},
   "source": [
    "### Stop\n",
    "\n",
    "You have completed Module 1-Lab 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
